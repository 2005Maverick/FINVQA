{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# HAAST: A Novel Multimodal Transformer Architecture\n# ==============================================================================\n# Complete implementation in a single file for Kaggle environments.\n#\n# This script includes:\n# 1.  All necessary library imports, now with tqdm.\n# 2.  Centralized configuration management.\n# 3.  Modality-specific encoders (Text, Vision) with gating.\n# 4.  Core HAA Block components, including Associative Attention and MoE.\n# 5.  The main HAAST model assembling all modules.\n# 6.  Output heads for different tasks.\n# 7.  A training and evaluation class with progress bars, mixed-precision, and pruning.\n# 8.  A main execution block to run a full training and testing pipeline on CIFAR-10.\n# ==============================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.nn.utils import prune\nfrom torch.utils.data import DataLoader\nfrom torch.utils.checkpoint import checkpoint\nimport math\nimport yaml\nimport unittest\nfrom einops.layers.torch import Rearrange\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\nfrom tqdm import tqdm # Import tqdm\n\n# ==============================================================================\n# 1. CONFIGURATION (Originally haast/config.py)\n# ==============================================================================\ndef get_config():\n    \"\"\"Returns the default configuration for the HAAST model.\"\"\"\n    # Aligned dimensions to multiples of 32 for TensorCore efficiency\n    return {\n        # Model Architecture\n        \"d_model\": 768,\n        \"n_blocks\": 12,\n        \"n_heads\": 12,\n        \"vocab_size\": 32000,\n        \"experts_per_block\": 4,\n        \"top_k_experts\": 2,\n        \"global_sync_interval\": 4,\n\n        # Vision Encoder\n        \"image_size\": 224,\n        \"patch_size\": 16,\n\n        # Training & Efficiency\n        \"dropout\": 0.1,\n        \"optimizer\": \"AdamW\",\n        \"lr_schedule\": {\n            \"warmup_pct\": 0.05,\n            \"decay\": \"cosine\"\n        },\n        \"pruning_amount\": 0.0, # Set to > 0 to enable pruning during training\n        \"mixed_precision\": True,\n        \"gradient_checkpointing\": True,\n\n        # Task Specific\n        \"num_classes\": 10, # For CIFAR-10 classification\n\n        # Not used in this version but part of the original architecture\n        \"kernel_features\": 256,\n        \"memory_capacity\": 1024,\n        \"early_exit_threshold\": 0.9,\n        \"sparse_attention_k\": 32,\n    }\n\n# ==============================================================================\n# 2. SHARED UTILITIES & LAYERS\n# ==============================================================================\nclass GatingLayer(nn.Module):\n    \"\"\"A sparse gating layer to prune activations.\"\"\"\n    def __init__(self, d_model: int, prune_ratio: float = 0.3):\n        super().__init__()\n        self.gate = nn.Linear(d_model, d_model)\n        self.prune_ratio = prune_ratio\n\n    def forward(self, x):\n        gate_scores = torch.sigmoid(self.gate(x))\n        if self.training and self.prune_ratio > 0:\n            # Keep top (1 - prune_ratio) % of activations\n            k = int(gate_scores.numel() * (1 - self.prune_ratio))\n            # Find the threshold for the top-k values\n            threshold, _ = torch.kthvalue(gate_scores.view(-1), k)\n            mask = gate_scores >= threshold\n        else:\n            # During inference, use a fixed threshold or no pruning\n            mask = gate_scores > self.prune_ratio\n        return x * mask\n\n# ==============================================================================\n# 3. ENCODERS (Originally haast/encoders/)\n# ==============================================================================\n# --- 3.1 Text Encoder ---\nclass HarmonicPositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [batch_size, seq_len, d_model]\n        \"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass TextEncoder(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = HarmonicPositionalEncoding(d_model, dropout)\n        self.gate = GatingLayer(d_model)\n        self.d_model = d_model\n\n    def forward(self, src: torch.Tensor) -> torch.Tensor:\n        # src shape: [B, T]\n        x = self.embedding(src) * math.sqrt(self.d_model)\n        x = self.pos_encoder(x)\n        x = self.gate(x)\n        return x # Output: [B, T, d_model]\n\n# --- 3.2 Vision Encoder ---\nclass VisionEncoder(nn.Module):\n    def __init__(self, image_size: int, patch_size: int, d_model: int, channels: int = 3):\n        super().__init__()\n        image_height, image_width = image_size, image_size\n        patch_height, patch_width = patch_size, patch_size\n        assert image_height % patch_height == 0 and image_width % patch_width == 0\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_height * patch_width\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(patch_dim, d_model),\n        )\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n        self.gate = GatingLayer(d_model)\n\n    def forward(self, img: torch.Tensor) -> torch.Tensor:\n        # img shape: [B, 3, H, W]\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.gate(x)\n        return x # Output: [B, num_patches + 1, d_model]\n\n# ==============================================================================\n# 4. UNIFIED LATENT PROJECTION (Originally haast/encoder_utils.py)\n# ==============================================================================\nclass UnifiedLatentProjection(nn.Module):\n    def __init__(self, d_model: int, num_modalities: int = 2):\n        super().__init__()\n        self.projection = nn.Linear(d_model, d_model)\n        self.norm = nn.LayerNorm(d_model)\n        self.fusion_weights = nn.Parameter(torch.ones(num_modalities))\n\n    def forward(self, *encoder_outputs: torch.Tensor, mode: str = 'concat') -> torch.Tensor:\n        if mode == 'concat':\n            x = torch.cat(encoder_outputs, dim=1)\n        elif mode in ('mean_pool_sum', 'weighted_pool_sum'):\n            pooled_outputs = [torch.mean(output, dim=1) for output in encoder_outputs]\n            if mode == 'weighted_pool_sum':\n                normalized_weights = F.softmax(self.fusion_weights, dim=0)\n                weighted_pooled = torch.stack([out * w for out, w in zip(pooled_outputs, normalized_weights)])\n                fused_pooled = torch.sum(weighted_pooled, dim=0)\n            else: # mean_pool_sum\n                fused_pooled = torch.stack(pooled_outputs, dim=0).sum(dim=0)\n            max_len = max(out.size(1) for out in encoder_outputs)\n            x = fused_pooled.unsqueeze(1).repeat(1, max_len, 1)\n        else:\n            raise ValueError(f\"Unknown or unsupported fusion mode: {mode}\")\n\n        x = self.projection(x)\n        x = self.norm(x)\n        return x\n\n# ==============================================================================\n# 5. CORE HAA BLOCK (Originally haast/blocks/)\n# ==============================================================================\n# --- 5.1 Associative Attention ---\nclass AssociativeAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        \n        self.to_qkv = nn.Linear(d_model, d_model * 3, bias=False)\n        self.to_out = nn.Linear(d_model, d_model)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, n, _, h = *x.shape, self.n_heads\n        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: Rearrange('b n (h d) -> b h n d', h=h)(t), (q, k, v))\n        \n        beta = self.head_dim ** -0.5\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * beta\n        attn = dots.softmax(dim=-1)\n        \n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = Rearrange('b h n d -> b n (h d)')(out)\n        return self.to_out(out)\n\n# --- 5.2 Mixture-of-Experts Feed-Forward ---\nclass MoEFeedForward(nn.Module):\n    def __init__(self, d_model: int, n_experts: int, top_k: int, dropout: float):\n        super().__init__()\n        self.router = nn.Linear(d_model, n_experts)\n        self.experts = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(d_model, 4 * d_model),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(4 * d_model, d_model)\n            ) for _ in range(n_experts)\n        ])\n        self.top_k = top_k\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, n, d = x.shape\n        x_flat = x.view(-1, d)\n        \n        router_logits = self.router(x_flat)\n        routing_weights, selected_experts = torch.topk(\n            F.softmax(router_logits, dim=-1), self.top_k, dim=-1\n        )\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n        \n        final_output = torch.zeros_like(x_flat)\n        expert_mask = F.one_hot(selected_experts, num_classes=len(self.experts)).permute(2, 0, 1)\n\n        for i, expert in enumerate(self.experts):\n            idx, top_x = torch.where(expert_mask[i])\n            if top_x.shape[0] == 0:\n                continue\n            weights = routing_weights[idx, top_x].unsqueeze(-1)\n            final_output.index_add_(0, idx, weights * expert(x_flat[idx]))\n            \n        return final_output.view(b, n, d)\n\n# --- 5.3 Full HAA Block ---\nclass HAABlock(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        d_model = config['d_model']\n        n_heads = config['n_heads']\n        dropout = config['dropout']\n        \n        self.use_checkpoint = config.get(\"gradient_checkpointing\", True)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.assoc_attn = AssociativeAttention(d_model, n_heads)\n        \n        self.norm_moe = nn.LayerNorm(d_model)\n        self.moe_ffn = MoEFeedForward(d_model, config['experts_per_block'], config['top_k_experts'], dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def _checkpointable_forward(self, module, norm, data):\n        return module(norm(data))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Pre-norm architecture\n        attn_residual = x\n        if self.use_checkpoint and self.training:\n            attn_out = checkpoint(self._checkpointable_forward, self.assoc_attn, self.norm1, x)\n        else:\n            attn_out = self.assoc_attn(self.norm1(x))\n        x = attn_residual + self.dropout(attn_out)\n        \n        moe_residual = x\n        if self.use_checkpoint and self.training:\n            moe_out = checkpoint(self._checkpointable_forward, self.moe_ffn, self.norm_moe, x)\n        else:\n            moe_out = self.moe_ffn(self.norm_moe(x))\n        x = moe_residual + self.dropout(moe_out)\n        \n        return x\n\n# ==============================================================================\n# 6. GLOBAL SYNC LAYER (Originally haast/sync.py)\n# ==============================================================================\nclass GlobalSync(nn.Module):\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.gate = nn.Sequential(nn.Linear(d_model, 1), nn.Sigmoid())\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        global_summary = torch.mean(x, dim=1, keepdim=True)\n        gate_val = self.gate(global_summary)\n        return x * gate_val\n\n# ==============================================================================\n# 7. OUTPUT HEADS (Originally haast/heads/)\n# ==============================================================================\nclass ClassificationHead(nn.Module):\n    def __init__(self, d_model: int, num_classes: int):\n        super().__init__()\n        self.norm = nn.LayerNorm(d_model)\n        self.classifier = nn.Linear(d_model, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        cls_token_representation = x[:, 0]\n        return self.classifier(self.norm(cls_token_representation))\n\nclass LanguageModelHead(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.norm = nn.LayerNorm(d_model)\n        self.decoder = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.decoder(self.norm(x))\n\n# ==============================================================================\n# 8. HAAST MAIN MODEL (Originally haast/model.py)\n# ==============================================================================\nclass HAAST(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        self.config = config\n        \n        # --- Encoders ---\n        self.text_encoder = TextEncoder(config['vocab_size'], config['d_model'], config['dropout'])\n        self.vision_encoder = VisionEncoder(config['image_size'], config['patch_size'], config['d_model'])\n        \n        # --- Fusion Layer ---\n        self.latent_projection = UnifiedLatentProjection(config['d_model'])\n        \n        # --- Core Blocks ---\n        self.blocks = nn.ModuleList([\n            HAABlock(config) for _ in range(config['n_blocks'])\n        ])\n        \n        # --- Sync Layer ---\n        self.global_sync = GlobalSync(config['d_model'])\n        self.sync_interval = config['global_sync_interval']\n        \n        # --- Output Head ---\n        self.output_head = ClassificationHead(config['d_model'], num_classes=config['num_classes'])\n\n    def forward(self, text_input=None, vision_input=None) -> torch.Tensor:\n        encoder_outputs = []\n        if text_input is not None:\n            encoder_outputs.append(self.text_encoder(text_input))\n        if vision_input is not None:\n            encoder_outputs.append(self.vision_encoder(vision_input))\n        \n        if not encoder_outputs:\n            raise ValueError(\"At least one input modality must be provided.\")\n            \n        x = self.latent_projection(*encoder_outputs, mode='concat')\n        \n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            if (i + 1) % self.sync_interval == 0:\n                x = self.global_sync(x)\n                \n        return self.output_head(x)\n\n# ==============================================================================\n# 9. DATA LOADING FOR REAL DATASET (CIFAR-10)\n# ==============================================================================\ndef get_dataloaders(image_size: int, batch_size: int):\n    \"\"\"Downloads and prepares CIFAR-10 dataloaders.\"\"\"\n    print(\"Preparing CIFAR-10 dataloaders...\")\n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize for CIFAR-10\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                            download=True, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                              shuffle=True, num_workers=2, pin_memory=True)\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                           download=True, transform=transform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                             shuffle=False, num_workers=2, pin_memory=True)\n    print(\"âœ… Dataloaders ready.\")\n    return trainloader, testloader\n\n# ==============================================================================\n# 10. TRAINING & EFFICIENCY (Originally haast/training/)\n# ==============================================================================\nclass Trainer:\n    def __init__(self, model: nn.Module, optimizer, scheduler, device, config: dict):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = device\n        self.config = config\n        self.scaler = GradScaler(enabled=config.get('mixed_precision', True))\n        \n    def train_epoch(self, dataloader, epoch_num, total_epochs):\n        self.model.train()\n        total_loss = 0\n        \n        # Wrap dataloader with tqdm for a progress bar\n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch_num}/{total_epochs} [Training]\")\n        \n        # FIX: Use enumerate to get the batch index `i`\n        for i, batch in enumerate(progress_bar):\n            images, targets = batch\n            images = images.to(self.device, non_blocking=True)\n            targets = targets.to(self.device, non_blocking=True)\n\n            self.optimizer.zero_grad(set_to_none=True)\n\n            with autocast(enabled=self.scaler.is_enabled()):\n                # We are training on vision-only, so pass only vision_input\n                outputs = self.model(vision_input=images)\n                loss = F.cross_entropy(outputs, targets)\n\n            self.scaler.scale(loss).backward()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            total_loss += loss.item()\n            \n            # FIX: Calculate average loss using the batch index (i + 1)\n            progress_bar.set_postfix(loss=total_loss/(i + 1))\n\n        # Pruning is done once per epoch after the weights have been updated\n        self.prune_model()\n        return total_loss / len(dataloader)\n\n    def evaluate(self, dataloader):\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        # Wrap dataloader with tqdm for a progress bar\n        progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n\n        with torch.no_grad():\n            for batch in progress_bar:\n                images, targets = batch\n                images = images.to(self.device, non_blocking=True)\n                targets = targets.to(self.device, non_blocking=True)\n\n                with autocast(enabled=self.scaler.is_enabled()):\n                    outputs = self.model(vision_input=images)\n                    loss = F.cross_entropy(outputs, targets)\n\n                total_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n\n                # Update progress bar with running accuracy\n                progress_bar.set_postfix(accuracy=f\"{100 * correct / total:.2f}%\")\n\n        avg_loss = total_loss / len(dataloader)\n        accuracy = 100 * correct / total\n        return avg_loss, accuracy\n\n\n    def prune_model(self):\n        pruning_amount = self.config.get('pruning_amount', 0.0)\n        if not (pruning_amount > 0 and self.model.training):\n            return\n\n        print(f\"Applying {pruning_amount:.1%} unstructured L1 pruning...\")\n        for module in self.model.modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                prune.l1_unstructured(module, name=\"weight\", amount=pruning_amount)\n                # To make pruning permanent and remove re-parameterization overhead\n                prune.remove(module, 'weight')\n        print(\"Pruning complete.\")\n\n# ==============================================================================\n# 11. MAIN EXECUTION BLOCK FOR TRAINING & EVALUATION\n# ==============================================================================\ndef main():\n    \"\"\"\n    Main function to run the HAAST training and evaluation pipeline.\n    \"\"\"\n    print(\"ðŸš€ Starting HAAST Model Training & Evaluation on CIFAR-10...\")\n\n    # --- Training Configuration ---\n    # For a full run, use more epochs. For a quick test, 3-5 is fine.\n    EPOCHS = 10\n    BATCH_SIZE = 32 # Adjust based on your GPU memory\n    LEARNING_RATE = 1e-4\n\n    # --- Get Model Config and Device ---\n    config = get_config()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Reduce model size for quick CPU testing if needed\n    if device.type == 'cpu':\n        print(\"âš ï¸ CPU detected. Downscaling model and data for quick testing.\")\n        config['d_model'] = 128\n        config['n_heads'] = 4\n        config['n_blocks'] = 3\n        config['experts_per_block'] = 2\n        BATCH_SIZE = 16 # Reduce batch size for CPU memory\n\n    # --- Data Loading ---\n    train_loader, test_loader = get_dataloaders(config['image_size'], BATCH_SIZE)\n\n    # --- Model Initialization ---\n    try:\n        model = HAAST(config).to(device)\n        print(\"âœ… Model initialization successful.\")\n        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Model Parameters: {num_params / 1e6:.2f}M\")\n    except Exception as e:\n        print(f\"âŒ Model initialization failed: {e}\")\n        return\n\n    # --- Optimizer and Scheduler ---\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n    \n    # --- Trainer Initialization ---\n    trainer = Trainer(model, optimizer, scheduler, device, config)\n\n    # --- Training & Evaluation Loop ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"ðŸ”¥ Starting Training ðŸ”¥\")\n    print(\"=\"*50)\n\n    for epoch in range(1, EPOCHS + 1):\n        start_time = time.time()\n\n        # Train one epoch\n        train_loss = trainer.train_epoch(train_loader, epoch, EPOCHS)\n        \n        # Evaluate on the test set\n        test_loss, test_accuracy = trainer.evaluate(test_loader)\n        \n        # Step the scheduler\n        if trainer.scheduler:\n            trainer.scheduler.step()\n\n        epoch_duration = time.time() - start_time\n        \n        # The progress bar provides running feedback, so we print a summary at the end of the epoch\n        print(f\"Epoch {epoch}/{EPOCHS} Summary | Time: {epoch_duration:.2f}s\")\n        print(f\"  -> Train Loss: {train_loss:.4f}\")\n        print(f\"  -> Test Loss:  {test_loss:.4f}\")\n        print(f\"  -> Test Accuracy: {test_accuracy:.2f}%\")\n        print(f\"  -> Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n        print(\"-\" * 50)\n\n\n    print(\"\\nðŸŽ‰ Training finished!\")\n    final_loss, final_accuracy = trainer.evaluate(test_loader)\n    print(f\"Final Test Accuracy: {final_accuracy:.2f}%\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T12:17:04.701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import List, Optional, Tuple\nimport math\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport numpy as np\n\n# ==========================================================================================\n# |                                  Vision Transformer (ViT) Architecture                 |\n# ==========================================================================================\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"\n    Splits an image into patches, flattens them, and projects them into a higher dimension.\n    Also adds a learnable [CLS] token and positional embeddings.\n    \"\"\"\n    def __init__(self, image_size: int, patch_size: int, in_channels: int, embed_dim: int):\n        super().__init__()\n        # Ensure image dimensions are divisible by patch size\n        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n\n        # Calculate number of patches along one side\n        self.num_patches_side = image_size // patch_size\n        # Total number of patches\n        self.num_patches = self.num_patches_side * self.num_patches_side\n\n        # Convolutional layer to extract patches and project them\n        # Kernel size and stride equal to patch_size effectively create non-overlapping patches\n        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n        # Learnable [CLS] token for classification\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        \n        # Learnable positional embeddings for each patch + CLS token\n        self.position_embeddings = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n\n    def forward(self, x: Tensor) -> Tensor:\n        B, C, H, W = x.shape\n\n        # Validate input image size\n        if H != self.image_size or W != self.image_size or C != self.in_channels:\n            raise ValueError(\n                f\"Input image size ({B}x{C}x{H}x{W}) does not match model's configured size \"\n                f\"(Bx{self.in_channels}x{self.image_size}x{self.image_size}).\"\n            )\n\n        # Apply convolution to get patch embeddings\n        # Output shape will be (B, embed_dim, num_patches_side, num_patches_side)\n        x = self.projection(x)\n        \n        # Flatten the spatial dimensions to get a sequence of patch embeddings\n        # Output shape will be (B, embed_dim, num_patches)\n        x = x.flatten(2) # Flatten HxW into a single dimension\n        x = x.transpose(1, 2) # Transpose to (B, num_patches, embed_dim)\n\n        # Expand CLS token for the current batch size\n        cls_tokens = self.cls_token.expand(B, -1, -1) # (B, 1, embed_dim)\n\n        # Concatenate CLS token with patch embeddings\n        x = torch.cat((cls_tokens, x), dim=1) # (B, num_patches + 1, embed_dim)\n\n        # Add positional embeddings\n        x += self.position_embeddings\n\n        return x\n\nclass MultiHeadSelfAttention(nn.Module):\n    \"\"\"\n    Multi-Head Self-Attention mechanism.\n    Allows the model to jointly attend to information from different representation\n    subspaces at different positions.\n    \"\"\"\n    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads # Dimension of each attention head\n        self.scale = self.head_dim ** -0.5 # Scaling factor for dot product attention\n\n        # Linear layers for Query, Key, Value projections\n        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -> Tensor:\n        B, N, C = x.shape # Batch size, Sequence length, Embedding dimension\n\n        # Project input to Q, K, V\n        # Shape: (B, N, embed_dim * 3) -> split into (B, N, embed_dim) for Q, K, V\n        qkv = self.qkv_proj(x).chunk(3, dim=-1)\n        \n        # Reshape Q, K, V for multi-head attention\n        # (B, N, embed_dim) -> (B, N, num_heads, head_dim) -> (B, num_heads, N, head_dim)\n        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n\n        # Calculate attention scores (Q @ K_T)\n        # (B, num_heads, N, head_dim) @ (B, num_heads, head_dim, N) -> (B, num_heads, N, N)\n        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Apply attention weights to Values (attn_weights @ V)\n        # (B, num_heads, N, N) @ (B, num_heads, N, head_dim) -> (B, num_heads, N, head_dim)\n        attn_output = attn_weights @ v\n\n        # Concatenate heads and project back to original embedding dimension\n        # (B, num_heads, N, head_dim) -> (B, N, num_heads, head_dim) -> (B, N, embed_dim)\n        attn_output = attn_output.transpose(1, 2).reshape(B, N, C)\n        \n        # Final linear projection\n        output = self.out_proj(attn_output)\n        output = self.dropout(output) # Apply dropout after output projection\n        return output\n\nclass MLP(nn.Module):\n    \"\"\"\n    A simple Multi-Layer Perceptron (Feed-Forward Network).\n    Applied to each token independently.\n    \"\"\"\n    def __init__(self, in_features: int, hidden_features: int, out_features: int, dropout: float = 0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"\n    A single Vision Transformer encoder block.\n    Consists of Multi-Head Self-Attention, Layer Normalization, and an MLP.\n    Includes residual connections.\n    \"\"\"\n    def __init__(self, embed_dim: int, num_heads: int, mlp_ratio: float = 4.0, dropout: float = 0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout=dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = MLP(embed_dim, mlp_hidden_dim, embed_dim, dropout=dropout)\n\n    def forward(self, x: Tensor) -> Tensor:\n        # LayerNorm -> Attention -> Add residual\n        x = x + self.attn(self.norm1(x))\n        \n        # LayerNorm -> MLP -> Add residual\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass VisionTransformer(nn.Module):\n    \"\"\"\n    The complete Vision Transformer model for image classification.\n    \"\"\"\n    def __init__(self, image_size: int, patch_size: int, in_channels: int, num_classes: int,\n                 embed_dim: int, num_layers: int, num_heads: int, mlp_ratio: float = 4.0,\n                 dropout: float = 0.1):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n        \n        # Stack of Transformer encoder blocks\n        self.encoder = nn.Sequential(\n            *[TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout) \n              for _ in range(num_layers)]\n        )\n        \n        self.final_norm = nn.LayerNorm(embed_dim) # Final LayerNorm before classification head\n        self.classification_head = nn.Linear(embed_dim, num_classes) # Classification head\n\n        self.apply(self._init_weights) # Apply custom weight initialization\n\n    def _init_weights(self, module):\n        \"\"\"Custom weight initialization for better training stability.\"\"\"\n        if isinstance(module, nn.Linear):\n            # Glorot / Xavier normal initialization for linear layers\n            torch.nn.init.xavier_normal_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0)\n        elif isinstance(module, nn.LayerNorm):\n            nn.init.constant_(module.bias, 0)\n            nn.init.constant_(module.weight, 1.0)\n        elif isinstance(module, PatchEmbedding):\n            # Initialize positional embeddings and CLS token\n            nn.init.trunc_normal_(module.position_embeddings, std=0.02)\n            nn.init.trunc_normal_(module.cls_token, std=0.02)\n            # For patch projection (Conv2d), standard initialization is usually fine\n            if isinstance(module.projection, nn.Conv2d):\n                nn.init.kaiming_normal_(module.projection.weight, mode='fan_out', nonlinearity='relu')\n                if module.projection.bias is not None:\n                    nn.init.constant_(module.projection.bias, 0)\n\n\n    def forward(self, x: Tensor) -> Tensor:\n        # 1. Patch Embedding + Positional Encoding\n        x = self.patch_embedding(x) # (B, num_patches + 1, embed_dim)\n\n        # 2. Transformer Encoder Blocks\n        x = self.encoder(x) # (B, num_patches + 1, embed_dim)\n\n        # 3. Take the [CLS] token's output for classification\n        cls_token_output = self.final_norm(x[:, 0]) # (B, embed_dim)\n\n        # 4. Classification Head\n        logits = self.classification_head(cls_token_output) # (B, num_classes)\n        return logits\n\n# ==========================================================================================\n# |                                  Training and Evaluation                               |\n# ==========================================================================================\n\nif __name__ == \"__main__\":\n    torch.autograd.set_detect_anomaly(True)\n\n    print(\"ðŸš€ Starting Vision Transformer Training on CIFAR-10...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # ViT Model Parameters (tuned for CIFAR-10, can be adjusted)\n    vit_image_size = 32\n    vit_patch_size = 4 # Smaller patches mean more tokens, potentially better for small images but more compute\n    vit_in_channels = 3\n    vit_num_classes = 10\n    vit_embed_dim = 256 # Dimension of token embeddings\n    vit_num_layers = 6  # Number of Transformer encoder blocks\n    vit_num_heads = 8   # Number of attention heads\n    vit_mlp_ratio = 4.0 # Expansion factor for the MLP\n    vit_dropout = 0.1\n\n    print(\"\\n[STEP 1/5] Loading and preprocessing CIFAR-10 dataset...\")\n    # CIFAR-10 normalization values (mean and std for RGB channels)\n    cifar10_mean = (0.4914, 0.4822, 0.4465)\n    cifar10_std = (0.2471, 0.2435, 0.2616)\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(cifar10_mean, cifar10_std)\n    ])\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True) # pin_memory for faster GPU transfer\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n    print(\"âœ… Dataset loaded.\")\n\n    print(\"\\n[STEP 2/5] Initializing the Vision Transformer model...\")\n    model = VisionTransformer(\n        image_size=vit_image_size,\n        patch_size=vit_patch_size,\n        in_channels=vit_in_channels,\n        num_classes=vit_num_classes,\n        embed_dim=vit_embed_dim,\n        num_layers=vit_num_layers,\n        num_heads=vit_num_heads,\n        mlp_ratio=vit_mlp_ratio,\n        dropout=vit_dropout\n    ).to(device)\n    print(f\"âœ… Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n\n    print(\"\\n[STEP 3/5] Setting up loss function and optimizer...\")\n    criterion = nn.CrossEntropyLoss()\n    # AdamW is generally a good choice for Transformers\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01) # Slightly higher LR than MathNet\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) # More epochs for CIFAR-10\n    print(\"âœ… Setup complete.\")\n\n    print(\"\\n[STEP 4/5] Starting training loop...\")\n    num_epochs = 50 # Increased epochs for CIFAR-10\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=True)\n        \n        for i, data in enumerate(progress_bar):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            if torch.isnan(loss):\n                print(\"ðŸš¨ Loss is NaN! Stopping training.\")\n                break\n                \n            loss.backward()\n            # Gradient clipping to prevent exploding gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) \n            optimizer.step()\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n            \n            progress_bar.set_postfix({\n                'loss': f'{loss.item():.4f}', \n                'lr': f'{scheduler.get_last_lr()[0]:.1e}',\n                'acc': f'{100 * correct_train / total_train:.2f}%'\n            })\n        \n        scheduler.step()\n        if torch.isnan(loss):\n            break\n\n    print(\"âœ… Finished Training.\")\n\n    print(\"\\n[STEP 5/5] Evaluating model on test data...\")\n    model.eval()\n    correct_test = 0\n    total_test = 0\n    with torch.no_grad():\n        for data in tqdm(testloader, desc=\"Evaluating\", leave=True):\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total_test += labels.size(0)\n            correct_test += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct_test / total_test\n    print(f'âœ… Evaluation Complete. Accuracy on 10,000 test images: {accuracy:.2f} %')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T12:17:35.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}